# Manipulation Tools Configuration
# Defines tools for robot manipulation capabilities using machine learning models

categories:
  manipulation:
    description: "Machine learning-powered object manipulation tools"
    tools:
      - name: object_manipulation
        description: "Control the robot to manipulate objects based on a task description using machine learning models"
        hardware_required: true
        parameters:
          - name: task
            type: string
            description: "Textual description of the task to perform (e.g., 'pick up the cup', 'open the drawer')"
      - name: manipulation_workflow
        description: "Orchestrate a complete manipulation sequence with the robot arm"
        hardware_required: true
        parameters:
          - name: reach_position
            type: list
            description: "List [x, y, z, theta_x, theta_y, theta_z, gripper] representing the target position to reach"
          - name: lift_position
            type: list
            description: "List [x, y, z, theta_x, theta_y, theta_z, gripper] representing the position to lift the object"
          - name: place_position
            type: list
            description: "List [x, y, z, theta_x, theta_y, theta_z, gripper] representing the position to place the object"
          - name: grasping_task
            type: string
            description: "String describing the grasping task (e.g., 'grasp cup', 'grasp blender')"

# Tool prompts for LLM system prompts
tool_prompts:
  object_manipulation:
    name: object_manipulation
    description: "Use deep learning model to perform precise manipulation of objects. Your input must be one of the following:[grasp cup from above,grasp cup,grasp blender]"
    parameters:
      - name: task
        description: "A clear, concise description of the manipulation task (e.g., 'grasp cup', 'grasp blender')"
    example: "To have the robot grasp an object: object_manipulation(task=\"grasp cup\")"
    result_example: |
      Object manipulation completed:
      - Task: grasp the cup
      - Duration: 10.25 seconds
      - Actions sent: 302
      - Success rate: 97.3%
      , 
  
  manipulation_workflow:
    name: manipulation_workflow
    description: "Execute a complete object manipulation workflow including reaching, grasping, lifting, and placing an object"
    parameters:
      - name: reach_position
        description: "List of values [x, y, z, theta_x, theta_y, theta_z, gripper] representing the target position to reach"
      - name: lift_position
        description: "List of values [x, y, z, theta_x, theta_y, theta_z, gripper] representing the position to lift the object"
      - name: place_position
        description: "List of values [x, y, z, theta_x, theta_y, theta_z, gripper] representing the position to place the object"
      - name: grasping_task
        description: "String describing the grasping task (e.g., 'grasp cup', 'grasp blender')"
    example: "To pick and place a cup: manipulation_workflow(reach_position=[0.4, -0.1, 0.3, 0.2, 0, 0.2, 0], lift_position=[0.4, -0.1, 0.5, 0, 0, 0.2, 0], place_position=[0.5, 0.2, 0.3, 0.2, 0, 0.2, 0], grasping_task=\"grasp cup\")"
    result_example: |
      Manipulation workflow completed successfully

# Usage guide for manipulation tools
manipulation_tools_guide: |
  The object manipulation tool provides high-level control of the robot arm using machine learning models.
  
  When to use object manipulation:
  - When you need the robot to perform complex, coordinated movements
  - When you need the robot to react to the environment in real-time
  - When you want to express a task in natural language instead of detailed coordinates
  
  Guidelines for using object manipulation:
  1. Provide clear, specific task descriptions (e.g., "pick up the blue cup" is better than "grab a cup")
  2. Tasks will run for a fixed duration (typically 10 seconds)
  3. The tool automatically uses both camera views and proprioceptive feedback
  4. The tool uses a machine learning model trained on similar tasks
  5. Monitor the success rate in the result to determine if the task succeeded
  
  Example tasks:
  - "pick up the red cup"
  - "open the drawer"
  - "place the spoon in the bowl"
  - "push the button"
  - "twist the lid off the jar"
  
  Limitations:
  - Performance depends on the quality of the underlying model
  - Tasks must be similar to what the model was trained on
  - Complex multi-step tasks may need to be broken down into simpler commands
  - The tool has a fixed execution time regardless of task complexity 
  
  The manipulation_workflow tool provides a higher-level orchestration of robotic manipulation:
  
  When to use manipulation_workflow:
  - When you need to perform a complete pick-and-place operation
  - When you want to combine precise positioning with ML-based grasping
  - When you have predefined positions for reaching, lifting, and placing objects
  
  Guidelines for using manipulation_workflow:
  1. First use get_action_positions() to identify available predefined positions
  2. Provide all positions as 7-value lists [x, y, z, theta_x, theta_y, theta_z, gripper]
  3. The grasping_task parameter accepts the same values as object_manipulation
  4. The workflow executes sequentially: reach → grasp → lift → place → release
  
  Example workflow:
  1. Get available positions with get_action_positions()
  2. Use manipulation_workflow with appropriate positions and grasping task 